{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9852fb87-7655-415a-b0ae-d9523fe5f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'BFS', 'graph': '[(1, 2), (1, 8), (1, 4), (1, 6), (1, 9), (1, 5), (2, 6), (2, 9), (8, 6), (8, 7), (8, 0), (4, 8), (6, 3), (6, 1), (6, 0), (9, 1), (9, 2), (9, 4), (5, 1), (5, 2), (5, 4), (0, 6), (7, 2), (7, 8), (7, 4), (7, 6)]', 'graph_adj': '{1: [2, 8, 4, 6, 9, 5],\\n2: [6, 9],\\n8: [6, 7, 0],\\n4: [8],\\n6: [3, 1, 0],\\n9: [1, 2, 4],\\n5: [1, 2, 4],\\n0: [6],\\n7: [2, 8, 4, 6],\\n3: []}', 'graph_nl': 'Node 1 is connected to nodes 2, 8, 4, 6, 9, 5.\\nNode 2 is connected to nodes 6, 9.\\nNode 8 is connected to nodes 6, 7, 0.\\nNode 4 is connected to node 8.\\nNode 6 is connected to nodes 3, 1, 0.\\nNode 9 is connected to nodes 1, 2, 4.\\nNode 5 is connected to nodes 1, 2, 4.\\nNode 0 is connected to node 6.\\nNode 7 is connected to nodes 2, 8, 4, 6.', 'num_nodes': 10, 'num_edges': 26, 'directed': True, 'question': 'Start from node 1, output a sequence of traversal in breadth-first search (BFS) order.', 'answer': '[1, 2, 8, 4, 6, 9, 5, 7, 0, 3]', 'steps': \"Let's run BFS step by step.\\nInitial state:\\nvisited: []. queue: [1].\\nVisit node 1.\\ntraversal: [1], queue: [2, 8, 4, 6, 9, 5].\\nVisit node 2.\\ntraversal: [1, 2], queue: [8, 4, 6, 9, 5].\\nVisit node 8.\\ntraversal: [1, 2, 8], queue: [4, 6, 9, 5, 7, 0].\\nVisit node 4.\\ntraversal: [1, 2, 8, 4], queue: [6, 9, 5, 7, 0].\\nVisit node 6.\\ntraversal: [1, 2, 8, 4, 6], queue: [9, 5, 7, 0, 3].\\nVisit node 9.\\ntraversal: [1, 2, 8, 4, 6, 9], queue: [5, 7, 0, 3].\\nVisit node 5.\\ntraversal: [1, 2, 8, 4, 6, 9, 5], queue: [7, 0, 3].\\nVisit node 7.\\ntraversal: [1, 2, 8, 4, 6, 9, 5, 7], queue: [0, 3].\\nVisit node 0.\\ntraversal: [1, 2, 8, 4, 6, 9, 5, 7, 0], queue: [3].\\nVisit node 3.\\ntraversal: [1, 2, 8, 4, 6, 9, 5, 7, 0, 3], queue: [].\\nThus, the sequence of traversal is \", 'choices': \"('[[1, 2, 8, 4, 6, 9, 5, 7, 0, 3], [1, 2, 8, 4, 6, 9, 0, 5, 3, 7], [1, 7, 5, 0, 3, 2, 8, 6, 9, 4], [1, 2, 8, 7, 0, 3, 4, 6, 9, 5]]',)\", 'label': 0, 'id': 881, 'translate_steps': None, '__index_level_0__': 361}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 读取 arrow 文件为 HuggingFace Dataset\n",
    "dataset = load_from_disk(\"/home/ud202281368/jupyterlab/GraphLLM-dev/GTG/data/dataset/hf_dataset/msra/BFS-int_id\")\n",
    "\n",
    "# 查看数据前几条\n",
    "print(dataset['dev'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034b8c7-5f30-427f-b4d8-b4cdd3831484",
   "metadata": {},
   "source": [
    "# in-domain datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bd8d2-7c27-4a13-8449-e73cedfbf1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "type_1 = [\"cycle-int_id\", \"connectivity-int_id\", \"edge-int_id\"]  # 判断类型\n",
    "\n",
    "type_2 = [\"BFS-int_id\", \"connected_component-int_id\", \"DFS-int_id\", \"euler_path-int_id\", \"hamiltonian_path-int_id\", \"neighbor-int_id\", \"predecessor-int_id\", \"shortest_path-int_id\", \"topological_sort-int_id\"]  # 遍历类型\n",
    "\n",
    "type_3 = [\"clustering_coefficient-int_id\", \"common_neighbor-int_id\", \"degree-int_id\", \"diameter-int_id\", \"jaccard-int_id\", \"maximum_flow-int_id\", \"MST-int_id\", \"page_rank-int_id\"]# 数字类型\n",
    "\n",
    "type_4 = [\"bipartite-int_id\"]  # 配对类型\n",
    "\n",
    "\n",
    "def convert_splits_to_json(dataset_path, output_dir, dataset):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face Arrow dataset with train/dev/test splits\n",
    "    into JSON files containing instruction/output pairs.\n",
    "    \"\"\"\n",
    "    # Load dataset from local disk\n",
    "    ds_dict = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each split\n",
    "    for split_name, split_ds in ds_dict.items():\n",
    "        records = []\n",
    "        for record in split_ds:\n",
    "            # Determine graph type\n",
    "            graph_type = 'directed' if record.get('directed', False) else 'undirected'\n",
    "\n",
    "            if dataset in type_1:\n",
    "                example = \"if your answer is Yes, please output <<<Yes>>> only.\"\n",
    "            elif dataset in type_2:\n",
    "                example = \"if your answer is [4,2,5,0], please output <<<[4,2,5,0]>>> only.\"\n",
    "            elif dataset in type_3:\n",
    "                example = \"if your answer is 0.3, please output <<<0.3>>> only.\"\n",
    "            elif dataset in type_4:\n",
    "                example = \"if your answer is (0, 4), (1, 8), (2, 3), please output <<<[(0, 4), (1, 8), (2, 3)]>>> only.\"\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            # Construct instruction string\n",
    "            instruction = f\"The following is a question related to the graph reasoning task {dataset.split('-')[0].upper()}. Please answer the question and format your final response as <<<ANSWER>>>. For example, {example} \\n\"\n",
    "            instruction += (\n",
    "                f\"Given a {graph_type} graph with the following connections:\\n\"\n",
    "                f\"{record['graph_nl']}\\n\"\n",
    "                f\"{record['question']}\"\n",
    "            )\n",
    "            \n",
    "            # Output is the 'answer' field\n",
    "            output = '<<<' + str(record['answer']) + '>>>'\n",
    "            \n",
    "            # Append to list\n",
    "            records.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": \"\",\n",
    "                \"output\": output,\n",
    "                \"id\": record['id']\n",
    "            })\n",
    "            if not str(output):\n",
    "                print(1)\n",
    "        \n",
    "        # Write to JSON file\n",
    "        out_file = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "        with open(out_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Saved {len(records)} records to {out_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9748b552-88a9-4ebf-b176-5fe8c7c65fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Saved 800 records to graphinstruct/BFS-int_id/train.json\n",
      "Saved 100 records to graphinstruct/BFS-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/BFS-int_id/test.json\n",
      "Saved 800 records to graphinstruct/bipartite-int_id/train.json\n",
      "Saved 100 records to graphinstruct/bipartite-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/bipartite-int_id/test.json\n",
      "Saved 800 records to graphinstruct/clustering_coefficient-int_id/train.json\n",
      "Saved 100 records to graphinstruct/clustering_coefficient-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/clustering_coefficient-int_id/test.json\n",
      "Saved 800 records to graphinstruct/common_neighbor-int_id/train.json\n",
      "Saved 100 records to graphinstruct/common_neighbor-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/common_neighbor-int_id/test.json\n",
      "Saved 800 records to graphinstruct/connected_component-int_id/train.json\n",
      "Saved 100 records to graphinstruct/connected_component-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/connected_component-int_id/test.json\n",
      "Saved 800 records to graphinstruct/connectivity-int_id/train.json\n",
      "Saved 100 records to graphinstruct/connectivity-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/connectivity-int_id/test.json\n",
      "Saved 800 records to graphinstruct/cycle-int_id/train.json\n",
      "Saved 100 records to graphinstruct/cycle-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/cycle-int_id/test.json\n",
      "Saved 800 records to graphinstruct/degree-int_id/train.json\n",
      "Saved 100 records to graphinstruct/degree-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/degree-int_id/test.json\n",
      "Saved 800 records to graphinstruct/DFS-int_id/train.json\n",
      "Saved 100 records to graphinstruct/DFS-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/DFS-int_id/test.json\n",
      "Saved 800 records to graphinstruct/diameter-int_id/train.json\n",
      "Saved 100 records to graphinstruct/diameter-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/diameter-int_id/test.json\n",
      "Saved 800 records to graphinstruct/edge-int_id/train.json\n",
      "Saved 100 records to graphinstruct/edge-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/edge-int_id/test.json\n",
      "Saved 800 records to graphinstruct/euler_path-int_id/train.json\n",
      "Saved 100 records to graphinstruct/euler_path-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/euler_path-int_id/test.json\n",
      "Saved 800 records to graphinstruct/hamiltonian_path-int_id/train.json\n",
      "Saved 100 records to graphinstruct/hamiltonian_path-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/hamiltonian_path-int_id/test.json\n",
      "Saved 800 records to graphinstruct/jaccard-int_id/train.json\n",
      "Saved 100 records to graphinstruct/jaccard-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/jaccard-int_id/test.json\n",
      "Saved 800 records to graphinstruct/maximum_flow-int_id/train.json\n",
      "Saved 100 records to graphinstruct/maximum_flow-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/maximum_flow-int_id/test.json\n",
      "Saved 800 records to graphinstruct/MST-int_id/train.json\n",
      "Saved 100 records to graphinstruct/MST-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/MST-int_id/test.json\n",
      "Saved 800 records to graphinstruct/neighbor-int_id/train.json\n",
      "Saved 100 records to graphinstruct/neighbor-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/neighbor-int_id/test.json\n",
      "Saved 800 records to graphinstruct/page_rank-int_id/train.json\n",
      "Saved 100 records to graphinstruct/page_rank-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/page_rank-int_id/test.json\n",
      "Saved 800 records to graphinstruct/predecessor-int_id/train.json\n",
      "Saved 100 records to graphinstruct/predecessor-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/predecessor-int_id/test.json\n",
      "Saved 800 records to graphinstruct/shortest_path-int_id/train.json\n",
      "Saved 100 records to graphinstruct/shortest_path-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/shortest_path-int_id/test.json\n",
      "Saved 800 records to graphinstruct/topological_sort-int_id/train.json\n",
      "Saved 100 records to graphinstruct/topological_sort-int_id/dev.json\n",
      "Saved 100 records to graphinstruct/topological_sort-int_id/test.json\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    \"BFS-int_id\",\n",
    "    \"bipartite-int_id\",\n",
    "    \"clustering_coefficient-int_id\",\n",
    "    \"common_neighbor-int_id\",\n",
    "    \"connected_component-int_id\",\n",
    "    \"connectivity-int_id\",\n",
    "    \"cycle-int_id\",\n",
    "    \"degree-int_id\",\n",
    "    \"DFS-int_id\",\n",
    "    \"diameter-int_id\",\n",
    "    \"edge-int_id\",\n",
    "    \"euler_path-int_id\",\n",
    "    \"hamiltonian_path-int_id\",\n",
    "    \"jaccard-int_id\",\n",
    "    \"maximum_flow-int_id\",\n",
    "    \"MST-int_id\",\n",
    "    \"neighbor-int_id\",\n",
    "    \"page_rank-int_id\",\n",
    "    \"predecessor-int_id\",\n",
    "    \"shortest_path-int_id\",\n",
    "    \"topological_sort-int_id\",\n",
    "]\n",
    "\n",
    "print(len(folders))\n",
    "\n",
    "for dataset in folders:\n",
    "    dataset_path = f'msra/{dataset}'\n",
    "    output_dir = f'graphinstruct/{dataset}'\n",
    "    convert_splits_to_json(dataset_path, output_dir, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684d3e2-68ee-4dc3-91cd-cd73d90efa37",
   "metadata": {},
   "source": [
    "# in-domain Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e4a37c-2e60-4978-9e59-eea540f61990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datasets import load_from_disk\n",
    "\n",
    "def convert_splits_to_csv(dataset_path, output_dir, dataset):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face Arrow dataset with train/dev/test splits\n",
    "    into JSON files containing instruction/output pairs.\n",
    "    \"\"\"\n",
    "    # Load dataset from local disk\n",
    "    ds_dict = load_from_disk(dataset_path)\n",
    "    for split_name, split_ds in ds_dict.items():\n",
    "        col_name = (list(split_ds.features.keys())[:-1])\n",
    "\n",
    "\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_file = os.path.join(output_dir, f'{dataset}.csv')\n",
    "    \n",
    "    # Iterate over each split\n",
    "    with open(csv_file, 'w', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(col_name)  # header\n",
    "        for split_name, split_ds in ds_dict.items():\n",
    "            records = []\n",
    "            for record in split_ds: \n",
    "                writer.writerow([record[col] for col in col_name])\n",
    "\n",
    "for dataset in folders:\n",
    "    dataset_path = f'msra/{dataset}'\n",
    "    output_dir = f'graphinstruct/{dataset}'\n",
    "    convert_splits_to_csv(dataset_path, output_dir, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c46edd-f892-441f-8ae0-6c86df00f7d8",
   "metadata": {},
   "source": [
    "# OOD Graph description languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8625b2-33bc-4477-b87d-5460e9343779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 records to generalization/edgelist/BFS-int_id/test.json\n",
      "Saved 100 records to generalization/adj/BFS-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/bipartite-int_id/test.json\n",
      "Saved 100 records to generalization/adj/bipartite-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/clustering_coefficient-int_id/test.json\n",
      "Saved 100 records to generalization/adj/clustering_coefficient-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/common_neighbor-int_id/test.json\n",
      "Saved 100 records to generalization/adj/common_neighbor-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/connected_component-int_id/test.json\n",
      "Saved 100 records to generalization/adj/connected_component-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/connectivity-int_id/test.json\n",
      "Saved 100 records to generalization/adj/connectivity-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/cycle-int_id/test.json\n",
      "Saved 100 records to generalization/adj/cycle-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/degree-int_id/test.json\n",
      "Saved 100 records to generalization/adj/degree-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/DFS-int_id/test.json\n",
      "Saved 100 records to generalization/adj/DFS-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/diameter-int_id/test.json\n",
      "Saved 100 records to generalization/adj/diameter-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/edge-int_id/test.json\n",
      "Saved 100 records to generalization/adj/edge-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/euler_path-int_id/test.json\n",
      "Saved 100 records to generalization/adj/euler_path-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/hamiltonian_path-int_id/test.json\n",
      "Saved 100 records to generalization/adj/hamiltonian_path-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/jaccard-int_id/test.json\n",
      "Saved 100 records to generalization/adj/jaccard-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/maximum_flow-int_id/test.json\n",
      "Saved 100 records to generalization/adj/maximum_flow-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/MST-int_id/test.json\n",
      "Saved 100 records to generalization/adj/MST-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/neighbor-int_id/test.json\n",
      "Saved 100 records to generalization/adj/neighbor-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/page_rank-int_id/test.json\n",
      "Saved 100 records to generalization/adj/page_rank-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/predecessor-int_id/test.json\n",
      "Saved 100 records to generalization/adj/predecessor-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/shortest_path-int_id/test.json\n",
      "Saved 100 records to generalization/adj/shortest_path-int_id/test.json\n",
      "Saved 100 records to generalization/edgelist/topological_sort-int_id/test.json\n",
      "Saved 100 records to generalization/adj/topological_sort-int_id/test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "type_1 = [\"cycle-int_id\", \"connectivity-int_id\", \"edge-int_id\"]  # 判断类型\n",
    "\n",
    "type_2 = [\"BFS-int_id\", \"connected_component-int_id\", \"DFS-int_id\", \"euler_path-int_id\", \"hamiltonian_path-int_id\", \"neighbor-int_id\", \"predecessor-int_id\", \"shortest_path-int_id\", \"topological_sort-int_id\"]  # 遍历类型\n",
    "\n",
    "type_3 = [\"clustering_coefficient-int_id\", \"common_neighbor-int_id\", \"degree-int_id\", \"diameter-int_id\", \"jaccard-int_id\", \"maximum_flow-int_id\", \"MST-int_id\", \"page_rank-int_id\"]# 数字类型\n",
    "\n",
    "type_4 = [\"bipartite-int_id\"]  # 配对类型\n",
    "\n",
    "\n",
    "def convert_splits_to_json_gdl(dataset_path, output_dir, dataset, gdl):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face Arrow dataset with train/dev/test splits\n",
    "    into JSON files containing instruction/output pairs.\n",
    "    \"\"\"\n",
    "    # Load dataset from local disk\n",
    "    ds_dict = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each split\n",
    "    for split_name, split_ds in ds_dict.items():\n",
    "        if split_name == 'test':\n",
    "            records = []\n",
    "            for record in split_ds:\n",
    "                # Determine graph type\n",
    "                graph_type = 'directed' if record.get('directed', False) else 'undirected'\n",
    "    \n",
    "                if dataset in type_1:\n",
    "                    example = \"if your answer is Yes, please output <<<Yes>>> only.\"\n",
    "                elif dataset in type_2:\n",
    "                    example = \"if your answer is [4,2,5,0], please output <<<[4,2,5,0]>>> only.\"\n",
    "                elif dataset in type_3:\n",
    "                    example = \"if your answer is 0.3, please output <<<0.3>>> only.\"\n",
    "                elif dataset in type_4:\n",
    "                    example = \"if your answer is (0, 4), (1, 8), (2, 3), please output <<<[(0, 4), (1, 8), (2, 3)]>>> only.\"\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                if gdl == 'adj':\n",
    "                    graph = record['graph_adj']\n",
    "                elif gdl == 'el':\n",
    "                    graph = record['graph']\n",
    "                \n",
    "                # Construct instruction string\n",
    "                instruction = f\"The following is a question related to the graph reasoning task {dataset.split('-')[0].upper()}. Please answer the question and format your final response as <<<ANSWER>>>. For example, {example} \\n\"\n",
    "                instruction += (\n",
    "                    f\"Given a {graph_type} graph with the following connections:\\n\"\n",
    "                    f\"{graph}\\n\"\n",
    "                    f\"{record['question']}\"\n",
    "                )\n",
    "                \n",
    "                # Output is the 'answer' field\n",
    "                output = '<<<' + str(record['answer']) + '>>>'\n",
    "                \n",
    "                # Append to list\n",
    "                records.append({\n",
    "                    \"instruction\": instruction,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": output,\n",
    "                    \"id\": record['id']\n",
    "                })\n",
    "                if not str(output):\n",
    "                    print(1)\n",
    "            \n",
    "            # Write to JSON file\n",
    "            out_file = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "            with open(out_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"Saved {len(records)} records to {out_file}\")\n",
    "\n",
    "\n",
    "folders = [\n",
    "    \"BFS-int_id\",\n",
    "    \"bipartite-int_id\",\n",
    "    \"clustering_coefficient-int_id\",\n",
    "    \"common_neighbor-int_id\",\n",
    "    \"connected_component-int_id\",\n",
    "    \"connectivity-int_id\",\n",
    "    \"cycle-int_id\",\n",
    "    \"degree-int_id\",\n",
    "    \"DFS-int_id\",\n",
    "    \"diameter-int_id\",\n",
    "    \"edge-int_id\",\n",
    "    \"euler_path-int_id\",\n",
    "    \"hamiltonian_path-int_id\",\n",
    "    \"jaccard-int_id\",\n",
    "    \"maximum_flow-int_id\",\n",
    "    \"MST-int_id\",\n",
    "    \"neighbor-int_id\",\n",
    "    \"page_rank-int_id\",\n",
    "    \"predecessor-int_id\",\n",
    "    \"shortest_path-int_id\",\n",
    "    \"topological_sort-int_id\",\n",
    "]\n",
    "\n",
    "for dataset in folders:\n",
    "    dataset_path = f'msra/{dataset}'\n",
    "    output_dir = f'generalization/edgelist/{dataset}'\n",
    "    convert_splits_to_json_gdl(dataset_path, output_dir, dataset, gdl='el')\n",
    "\n",
    "    output_dir = f'generalization/adj/{dataset}'\n",
    "    convert_splits_to_json_gdl(dataset_path, output_dir, dataset, gdl='adj')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa23aec-170a-4f09-8290-b2b4d9d6c5f7",
   "metadata": {},
   "source": [
    "# OOD letter id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bcc5262-628f-4584-967b-7989e8fa9226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/BFS-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/bipartite-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/clustering_coefficient-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/common_neighbor-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/connected_component-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/connectivity-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/cycle-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/degree-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/DFS-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/diameter-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/edge-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/euler_path-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/hamiltonian_path-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/jaccard-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/maximum_flow-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/MST-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/neighbor-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/page_rank-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/predecessor-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/shortest_path-letter_id/test.json\n",
      "✅ Saved 100 records to /home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/topological_sort-letter_id/test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 四类任务类型\n",
    "type_1 = [\"cycle-letter_id\", \"connectivity-letter_id\", \"edge-letter_id\"]  # 判断类型\n",
    "type_2 = [\"BFS-letter_id\", \"connected_component-letter_id\", \"DFS-letter_id\", \"euler_path-letter_id\",\n",
    "          \"hamiltonian_path-letter_id\", \"neighbor-letter_id\", \"predecessor-letter_id\",\n",
    "          \"shortest_path-letter_id\", \"topological_sort-letter_id\"]  # 遍历类型\n",
    "type_3 = [\"clustering_coefficient-letter_id\", \"common_neighbor-letter_id\", \"degree-letter_id\",\n",
    "          \"diameter-letter_id\", \"jaccard-letter_id\", \"maximum_flow-letter_id\", \"MST-letter_id\",\n",
    "          \"page_rank-letter_id\"]  # 数字类型\n",
    "type_4 = [\"bipartite-letter_id\"]  # 配对类型\n",
    "\n",
    "def convert_csv_to_json_letter(input_csv_path, output_dir, dataset):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with graph reasoning test data into JSON for instruction tuning.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    # 判断类型样例\n",
    "    if dataset in type_1:\n",
    "        example = \"if your answer is Yes, please output <<<Yes>>> only.\"\n",
    "    elif dataset in type_2:\n",
    "        example = \"if your answer is [KET, NER, OXR, ISY], please output <<<[KET, NER, OXR, ISY]>>> only.\"\n",
    "    elif dataset in type_3:\n",
    "        example = \"if your answer is 0.3, please output <<<0.3>>> only.\"\n",
    "    elif dataset in type_4:\n",
    "        example = \"if your answer is (KUE, IRW), (ODS, YTR), (PIA, LNV), please output <<<[(KUE, IRW), (ODS, YTR), (PIA, LNV)]>>> only.\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown dataset type: {dataset}\")\n",
    "\n",
    "    with open(input_csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            graph_type = 'directed' if row.get('directed', '').strip().lower() == 'true' else 'undirected'\n",
    "\n",
    "            instruction = f\"The following is a question related to the graph reasoning task {dataset.split('-')[0].upper()}. Please answer the question and format your final response as <<<ANSWER>>>. For example, {example} \\n\"\n",
    "            instruction += (\n",
    "                f\"Given a {graph_type} graph with the following connections:\\n\"\n",
    "                f\"{row['graph_nl']}\\n\"\n",
    "                f\"{row['question']}\"\n",
    "            )\n",
    "            output = f\"<<<{row['answer']}>>>\"\n",
    "            records.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": \"\",\n",
    "                \"output\": output,\n",
    "                \"id\": int(row['id']) if 'id' in row else None\n",
    "            })\n",
    "\n",
    "    out_file = os.path.join(output_dir, \"test.json\")\n",
    "    with open(out_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved {len(records)} records to {out_file}\")\n",
    "\n",
    "# 批量处理\n",
    "folders = [\n",
    "    \"BFS-letter_id\", \"bipartite-letter_id\", \"clustering_coefficient-letter_id\", \"common_neighbor-letter_id\",\n",
    "    \"connected_component-letter_id\", \"connectivity-letter_id\", \"cycle-letter_id\", \"degree-letter_id\",\n",
    "    \"DFS-letter_id\", \"diameter-letter_id\", \"edge-letter_id\", \"euler_path-letter_id\",\n",
    "    \"hamiltonian_path-letter_id\", \"jaccard-letter_id\", \"maximum_flow-letter_id\", \"MST-letter_id\",\n",
    "    \"neighbor-letter_id\", \"page_rank-letter_id\", \"predecessor-letter_id\", \"shortest_path-letter_id\",\n",
    "    \"topological_sort-letter_id\",\n",
    "]\n",
    "\n",
    "for dataset in folders:\n",
    "    input_csv = f\"/home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/{dataset}/{dataset}.csv\"  # 假设所有CSV文件都存在于这个目录\n",
    "    output_dir = f\"/home/ud202281368/jupyterlab/LLaMA-Factory/data/graphinstruct/{dataset}\"\n",
    "    convert_csv_to_json_letter(input_csv, output_dir, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c2e6e-e04c-4cd3-99e5-f21fd2aafc3f",
   "metadata": {},
   "source": [
    "# OOD Graph Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7e871bc-665e-4b91-93ec-686c248ac0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 100 records to generalization/mini/BFS-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/BFS-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/BFS-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/bipartite-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/bipartite-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/bipartite-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/clustering_coefficient-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/clustering_coefficient-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/clustering_coefficient-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/common_neighbor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/common_neighbor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/common_neighbor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/connected_component-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/connected_component-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/connected_component-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/connectivity-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/connectivity-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/connectivity-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/cycle-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/cycle-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/cycle-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/degree-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/degree-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/degree-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/DFS-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/DFS-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/DFS-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/diameter-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/diameter-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/diameter-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/edge-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/edge-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/edge-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/euler_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/euler_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/euler_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/hamiltonian_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/hamiltonian_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/hamiltonian_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/jaccard-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/jaccard-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/jaccard-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/maximum_flow-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/maximum_flow-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/maximum_flow-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/MST-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/MST-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/MST-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/neighbor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/neighbor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/neighbor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/page_rank-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/page_rank-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/page_rank-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/predecessor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/predecessor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/predecessor-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/shortest_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/shortest_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/shortest_path-int_id/test.json\n",
      "✅ Saved 100 records to generalization/mini/topological_sort-int_id/test.json\n",
      "✅ Saved 100 records to generalization/medium/topological_sort-int_id/test.json\n",
      "✅ Saved 100 records to generalization/large/topological_sort-int_id/test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "type_1 = [\"cycle-int_id\", \"connectivity-int_id\", \"edge-int_id\"]  # 判断类型\n",
    "\n",
    "type_2 = [\"BFS-int_id\", \"connected_component-int_id\", \"DFS-int_id\", \"euler_path-int_id\", \"hamiltonian_path-int_id\", \"neighbor-int_id\", \"predecessor-int_id\", \"shortest_path-int_id\", \"topological_sort-int_id\"]  # 遍历类型\n",
    "\n",
    "type_3 = [\"clustering_coefficient-int_id\", \"common_neighbor-int_id\", \"degree-int_id\", \"diameter-int_id\", \"jaccard-int_id\", \"maximum_flow-int_id\", \"MST-int_id\", \"page_rank-int_id\"]# 数字类型\n",
    "\n",
    "type_4 = [\"bipartite-int_id\"]  # 配对类型\n",
    "\n",
    "\n",
    "def convert_csv_to_json_size(input_csv_path, output_dir, dataset):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with graph reasoning test data into JSON for instruction tuning.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    # 判断类型样例\n",
    "    if dataset in type_1:\n",
    "        example = \"if your answer is Yes, please output <<<Yes>>> only.\"\n",
    "    elif dataset in type_2:\n",
    "        example = \"if your answer is [4,2,5,0], please output <<<[4,2,5,0]>>> only.\"\n",
    "    elif dataset in type_3:\n",
    "        example = \"if your answer is 0.3, please output <<<0.3>>> only.\"\n",
    "    elif dataset in type_4:\n",
    "        example = \"if your answer is (0, 4), (1, 8), (2, 3), please output <<<[(0, 4), (1, 8), (2, 3)]>>> only.\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    with open(input_csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            graph_type = 'directed' if row.get('directed', '').strip().lower() == 'true' else 'undirected'\n",
    "\n",
    "            instruction = f\"The following is a question related to the graph reasoning task {dataset.split('-')[0].upper()}. Please answer the question and format your final response as <<<ANSWER>>>. For example, {example} \\n\"\n",
    "            instruction += (\n",
    "                f\"Given a {graph_type} graph with the following connections:\\n\"\n",
    "                f\"{row['graph_nl']}\\n\"\n",
    "                f\"{row['question']}\"\n",
    "            )\n",
    "            output = f\"<<<{row['answer']}>>>\"\n",
    "            records.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": \"\",\n",
    "                \"output\": output,\n",
    "                \"id\": int(row['id']) if 'id' in row else None\n",
    "            })\n",
    "\n",
    "    out_file = os.path.join(output_dir, \"test.json\")\n",
    "    with open(out_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved {len(records)} records to {out_file}\")\n",
    "\n",
    "# 批量处理\n",
    "folders = [\n",
    "    \"BFS-int_id\",\n",
    "    \"bipartite-int_id\",\n",
    "    \"clustering_coefficient-int_id\",\n",
    "    \"common_neighbor-int_id\",\n",
    "    \"connected_component-int_id\",\n",
    "    \"connectivity-int_id\",\n",
    "    \"cycle-int_id\",\n",
    "    \"degree-int_id\",\n",
    "    \"DFS-int_id\",\n",
    "    \"diameter-int_id\",\n",
    "    \"edge-int_id\",\n",
    "    \"euler_path-int_id\",\n",
    "    \"hamiltonian_path-int_id\",\n",
    "    \"jaccard-int_id\",\n",
    "    \"maximum_flow-int_id\",\n",
    "    \"MST-int_id\",\n",
    "    \"neighbor-int_id\",\n",
    "    \"page_rank-int_id\",\n",
    "    \"predecessor-int_id\",\n",
    "    \"shortest_path-int_id\",\n",
    "    \"topological_sort-int_id\",\n",
    "]\n",
    "\n",
    "for dataset in folders:\n",
    "    input_csv = f\"/home/ud202281368/jupyterlab/LLaMA-Factory/data/generalization/mini/{dataset}/{dataset}.csv\"\n",
    "    output_dir = f'generalization/mini/{dataset}'\n",
    "    convert_csv_to_json_size(input_csv, output_dir, dataset)\n",
    "\n",
    "    input_csv = f\"/home/ud202281368/jupyterlab/LLaMA-Factory/data/generalization/medium/{dataset}/{dataset}.csv\"\n",
    "    output_dir = f'generalization/medium/{dataset}'\n",
    "    convert_csv_to_json_size(input_csv, output_dir, dataset)\n",
    "\n",
    "    input_csv = f\"/home/ud202281368/jupyterlab/LLaMA-Factory/data/generalization/large/{dataset}/{dataset}.csv\"\n",
    "    output_dir = f'generalization/large/{dataset}'\n",
    "    convert_csv_to_json_size(input_csv, output_dir, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9486d-978d-4474-bc0d-84ad0782e4ba",
   "metadata": {},
   "source": [
    "# step datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e20e3ed-46ca-41e8-a198-d5a8eeaab44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 10000 records to reasoning/small/BFS-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/bipartite-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/clustering_coefficient-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/common_neighbor-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/connected_component-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/connectivity-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/cycle-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/degree-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/DFS-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/diameter-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/edge-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/jaccard-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/maximum_flow-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/MST-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/neighbor-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/page_rank-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/predecessor-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/shortest_path-int_id/test.json\n",
      "✅ Saved 10000 records to reasoning/small/topological_sort-int_id/test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "type_1 = [\"cycle-int_id\", \"connectivity-int_id\", \"edge-int_id\"]  # 判断类型\n",
    "\n",
    "type_2 = [\"BFS-int_id\", \"connected_component-int_id\", \"DFS-int_id\", \"euler_path-int_id\", \"hamiltonian_path-int_id\", \"neighbor-int_id\", \"predecessor-int_id\", \"shortest_path-int_id\", \"topological_sort-int_id\"]  # 遍历类型\n",
    "\n",
    "type_3 = [\"clustering_coefficient-int_id\", \"common_neighbor-int_id\", \"degree-int_id\", \"diameter-int_id\", \"jaccard-int_id\", \"maximum_flow-int_id\", \"MST-int_id\", \"page_rank-int_id\"]# 数字类型\n",
    "\n",
    "type_4 = [\"bipartite-int_id\"]  # 配对类型\n",
    "\n",
    "\n",
    "def convert_csv_to_json_steps(input_csv_path, output_dir, dataset):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with graph reasoning test data into JSON for instruction tuning.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    # 判断类型样例\n",
    "    if dataset in type_1:\n",
    "        example = \"if your answer is Yes, please format your answer as <<<Yes>>>.\"\n",
    "    elif dataset in type_2:\n",
    "        example = \"if your answer is [4,2,5,0], please format your answer as <<<[4,2,5,0]>>>.\"\n",
    "    elif dataset in type_3:\n",
    "        example = \"if your answer is 0.3, please format your answer as <<<0.3>>>.\"\n",
    "    elif dataset in type_4:\n",
    "        example = \"if your answer is (0, 4), (1, 8), (2, 3), please format your answer as <<<[(0, 4), (1, 8), (2, 3)]>>>.\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    with open(input_csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            graph_type = 'directed' if row.get('directed', '').strip().lower() == 'true' else 'undirected'\n",
    "\n",
    "            instruction = f\"The following is a question related to the graph reasoning task {dataset.split('-')[0].upper()}. Please answer the question step by step and format your final response as <<<ANSWER>>>. For example, {example} \\n\"\n",
    "            instruction += (\n",
    "                f\"Given a {graph_type} graph with the following connections:\\n\"\n",
    "                f\"{row['graph_nl']}\\n\"\n",
    "                f\"{row['question']}\"\n",
    "            )\n",
    "            output = f\"{row['steps']} <<<{row['answer']}>>>\"\n",
    "            records.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": \"\",\n",
    "                \"output\": output,\n",
    "                \"id\": int(row['id']) if 'id' in row else None\n",
    "            })\n",
    "\n",
    "    out_file = os.path.join(output_dir, \"test.json\")\n",
    "    with open(out_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved {len(records)} records to {out_file}\")\n",
    "\n",
    "# 批量处理\n",
    "folders = [\n",
    "    \"BFS-int_id\",\n",
    "    \"bipartite-int_id\",\n",
    "    \"clustering_coefficient-int_id\",\n",
    "    \"common_neighbor-int_id\",\n",
    "    \"connected_component-int_id\",\n",
    "    \"connectivity-int_id\",\n",
    "    \"cycle-int_id\",\n",
    "    \"degree-int_id\",\n",
    "    \"DFS-int_id\",\n",
    "    \"diameter-int_id\",\n",
    "    \"edge-int_id\",\n",
    "    # \"euler_path-int_id\",\n",
    "    # \"hamiltonian_path-int_id\",\n",
    "    \"jaccard-int_id\",\n",
    "    \"maximum_flow-int_id\",\n",
    "    \"MST-int_id\",\n",
    "    \"neighbor-int_id\",\n",
    "    \"page_rank-int_id\",\n",
    "    \"predecessor-int_id\",\n",
    "    \"shortest_path-int_id\",\n",
    "    \"topological_sort-int_id\",\n",
    "]\n",
    "\n",
    "for dataset in folders:\n",
    "    input_csv = f\"/home/ud202281368/jupyterlab/LLaMA-Factory/data/reasoning/small/{dataset}/{dataset}.csv\"\n",
    "    output_dir = f'reasoning/small/{dataset}'\n",
    "    convert_csv_to_json_steps(input_csv, output_dir, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32d719-6c3c-451a-94b4-55cdb7d13740",
   "metadata": {},
   "source": [
    "# in domain reasoning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c3d55a-f6e3-494b-81af-febe9e94c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Saved 100 records to reasoning_test/common_neighbor-int_id/test.json\n",
      "Saved 100 records to reasoning_test/connectivity-int_id/test.json\n",
      "Saved 100 records to reasoning_test/degree-int_id/test.json\n",
      "Saved 100 records to reasoning_test/DFS-int_id/test.json\n",
      "Saved 100 records to reasoning_test/predecessor-int_id/test.json\n",
      "Saved 100 records to reasoning_test/shortest_path-int_id/test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "type_1 = [\"cycle-int_id\", \"connectivity-int_id\", \"edge-int_id\"]  # 判断类型\n",
    "\n",
    "type_2 = [\"BFS-int_id\", \"connected_component-int_id\", \"DFS-int_id\", \"euler_path-int_id\", \"hamiltonian_path-int_id\", \"neighbor-int_id\", \"predecessor-int_id\", \"shortest_path-int_id\", \"topological_sort-int_id\"]  # 遍历类型\n",
    "\n",
    "type_3 = [\"clustering_coefficient-int_id\", \"common_neighbor-int_id\", \"degree-int_id\", \"diameter-int_id\", \"jaccard-int_id\", \"maximum_flow-int_id\", \"MST-int_id\", \"page_rank-int_id\"]# 数字类型\n",
    "\n",
    "type_4 = [\"bipartite-int_id\"]  # 配对类型\n",
    "\n",
    "\n",
    "def convert_splits_to_json(dataset_path, output_dir, dataset):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face Arrow dataset with train/dev/test splits\n",
    "    into JSON files containing instruction/output pairs.\n",
    "    \"\"\"\n",
    "    # Load dataset from local disk\n",
    "    ds_dict = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each split\n",
    "    for split_name, split_ds in ds_dict.items():\n",
    "        records = []\n",
    "        if split_name == 'test':\n",
    "            for record in split_ds:\n",
    "                # Determine graph type\n",
    "                graph_type = 'directed' if record.get('directed', False) else 'undirected'\n",
    "    \n",
    "                if dataset in type_1:\n",
    "                    example = \"if your answer is Yes, please format your answer as <<<Yes>>>.\"\n",
    "                elif dataset in type_2:\n",
    "                    example = \"if your answer is [4,2,5,0], please format your answer as <<<[4,2,5,0]>>>.\"\n",
    "                elif dataset in type_3:\n",
    "                    example = \"if your answer is 0.3, please format your answer as <<<0.3>>>.\"\n",
    "                elif dataset in type_4:\n",
    "                    example = \"if your answer is (0, 4), (1, 8), (2, 3), please format your answer as <<<[(0, 4), (1, 8), (2, 3)]>>>.\"\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                # Construct instruction string\n",
    "                instruction = f\"The following is a question related to the graph reasoning task {dataset.split('-')[0].upper()}. Please answer the question step by step and format your final response as <<<ANSWER>>>. For example, {example} \\n\"\n",
    "                instruction += (\n",
    "                    f\"Given a {graph_type} graph with the following connections:\\n\"\n",
    "                    f\"{record['graph_nl']}\\n\"\n",
    "                    f\"{record['question']}\"\n",
    "                )\n",
    "                \n",
    "                output = f\"{record['steps']} <<<{record['answer']}>>>\"\n",
    "                \n",
    "                # Append to list\n",
    "                records.append({\n",
    "                    \"instruction\": instruction,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": output,\n",
    "                    \"id\": record['id']\n",
    "                })\n",
    "                if not str(output):\n",
    "                    print(1)\n",
    "            \n",
    "            # Write to JSON file\n",
    "            out_file = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "            with open(out_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"Saved {len(records)} records to {out_file}\")\n",
    "\n",
    "folders = [\n",
    "    \"common_neighbor-int_id\",\n",
    "    \"connectivity-int_id\",\n",
    "    \"degree-int_id\",\n",
    "    \"DFS-int_id\",\n",
    "    \"predecessor-int_id\",\n",
    "    \"shortest_path-int_id\",\n",
    "]\n",
    "\n",
    "print(len(folders))\n",
    "\n",
    "for dataset in folders:\n",
    "    dataset_path = f'msra/{dataset}'\n",
    "    output_dir = f'reasoning_test/{dataset}'\n",
    "    convert_splits_to_json(dataset_path, output_dir, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d77a28-6bba-4fd3-99b0-74962cf2e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
